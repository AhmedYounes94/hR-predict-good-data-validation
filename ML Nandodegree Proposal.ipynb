{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposal: Predicting Why Employees Leave\n",
    "\n",
    "## Domain Background\n",
    "According to the Bureau of Labor Statistics the median number of years that wage and salary workers had been with their current employer was 4.2 years in January 2016. While this number varies from industry to industry the story of an employee who sticks with one company for the entirety of a working life seems to be rather antiquated. \n",
    "This observation is combined with the fact, that \"employee turnover has been identified as a key issue for organizations because of its adverse impact on work place productivity and long term growth strategies\".[1] One of the key issues with a high employee turnover rate, combined with but reaching beyond cultural and sociological effects, is the cost associated with it. Research shows that the replacement cost for an hourly worker can be as high as 50 % of her annual salary. That number increases with the skillset of the worker up to 200% for senior-level workers and surges up to 400% for executive level positions.[2] It becomes obvious that the trend of shorter tenure in addition to high employee turnover rates can be a costly endeavour.\n",
    "\n",
    "Therefore it gets increasingly important to acquire the necessary tools for employers to understand where its workforce is standing. Additional insights from employer reports, scorecards as well as general statistical information can offer prediction values for companies when it comes to the longevity of jobs. In this research we are trying to predict the likelihood of an employee quitting based on available information and are trying to offer supervised machine learning methods in order to gain actionable insights on how to prevent a high employee turnover.\n",
    "\n",
    "## Problem Statement\n",
    "The problem to be solved is detecting the key elements of employee tenure and predicting whether an employee might be quitting her job.\tThis problem affects companies from all sizes. Although it can be said that based on statistics there are certain industry traits that have a higher tenure, relative speaking the tendency to switch jobs has increased over time. The boundaries of this problem of course are the underlying mechanisms of every individual company. These effects can not be generalized as in 'how to prevent' but more as in 'how to predict'. Company culture, direct reports and individual needs might turn out to be far more complicated than a list of features. However, if we fix this problem the path to a more successful and sustainable company culture is given. \n",
    "\n",
    "The setup of this research can be seen as a classification problem. Based on a set of features our solution should be able to determine if an employee quits or stays with a company. The core path to solving this problem is a supervised learning approach which will test the relationship between our dependent variables and our independent variable (did an employee leave or stay). A desirable problem solution can be quantified by correct detection of potential job quitters and the amount of created intervention opportunities.\n",
    "\n",
    "It shall be stated that a mechanism such as this can only work as a supplement to human interaction and empathy skills. \n",
    "\n",
    "## Datasets and Inputs\n",
    "Measuring employee satisfaction is a tough and highly interdependent task. There are a lot of different dimensions in play and turning them into quantifiably format (less to say machine-readable information) can pose a challenge. Information from employee reviews, demographics, balanced scorecards and key performance indicators can offer a first gateway to understanding an employee's desire to leave the company. \n",
    "\n",
    "In a dataset published on [kaggle](https://www.kaggle.com/ludobenistant/hr-analytics) we are offered information on current and former employees plus key features of their exmployment status. The dataset consists out of almost 15.000 data points and 10 variables. Unfortunately there is no available codebook besides a brief information on the available inputs. The inputs we will be using are:\n",
    "\n",
    "* Employee satisfaction level\n",
    "* Last evaluation\n",
    "* Number of projects\n",
    "* Average monthly hours\n",
    "* Time spent at the company\n",
    "* Whether they have had a work accident\n",
    "* Whether they have had a promotion in the last 5 years\n",
    "* Sales [describing the department]\n",
    "* Salary\n",
    "\n",
    "In addition the dataset provides an indicator whether an employee has left. This indicator will be our dependent variable that we try to predict.\n",
    "\n",
    "\n",
    "## Solution Statement\n",
    "First we'll do an in-depth exploratory analysis in an attempt to explain the underlying relationships in the data set. Afterwards we are trying to approach this problem on two dimensions by combining unsupervised and supervised learning algorithms:\n",
    "\n",
    "1. We'll be trying to apply two clustering algorithms namely [K-Means](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) and [Gaussian Mixture Model](http://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture) in an attempt to add more semantic to the employee set. Since we are operating in a sociological domain, the more we can derive from our feature set, the more human interaction and perception we can gather, the better. By clustering, we might be able to hand our supervised algorithm additional information for a better prediction down the road. In addition to this we might consider adding or engineering additional features from the existing feature set, depending on the questions we raise through this step.\n",
    "2. Afterwards we're going to use our features and additional cluster-information to predict whether an employee left or stayed. Given we established that we're dealing with labeled data and considering the amount of data points we're inclined to start with a [Linear Support Vector Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) or [Logistic Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) and work our way through a [KNeighbors Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) implementation. Afterwards we'll apply some even more sophisticated algorithms such as a [Support Vector Classifier with polynomial or RBF kernel](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) or ensemble methods like [Random Forest Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) or [eXtreme Gradient Boosting](https://pypi.python.org/pypi/xgboost).\n",
    "\n",
    "In addition we're trying to implement a model that will empower employers not only to detect, which of their own employees is about to leave the company but also determine which of the features contributes the most to a happy company culture. The success of our final solution however will be measured by its predicting accuracy on a held-out test set and the performance of our supervised learning approach.\n",
    "\n",
    "## Benchmark Model\n",
    "A lot of research and work has been performed to retain talent and decrease employee turnover rates. From corporate funded research and analytics services of companies such as [3] to management study classics such as [2] or more recent research [3] to name a few. \n",
    "\n",
    "There are valid methods for predicting employee turnover rate and results that can be taken as benchmark models. In this research we'll be using the machine learning approach of Punnose and Ajit as a benchmark. Their research performed predictive tasks on company information with an AUC (Area under the Curve) score of .86 on hold-out data as a best result using an XGBoost model.\n",
    "\n",
    "## Evaluation Metrics\n",
    "AUC (**A**rea **u**nder the **C**urve) is a common evaluation metric for binary classification problems. Its value is between 0 to 1 and describes the accuracy of a binary classification based on its true positive values. As described [here](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve), an area under the curve score is equal to the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one.\n",
    "\n",
    "To put this more into perspective, [kaggle](https://www.kaggle.com/wiki/AreaUnderCurve) describes a way of viewing AUC as a plot of the true positive rate vs. the false positive rate where the threshold value for classifying an item as 0 or is increased from 0 to 1. If the classifier is very good, the true positive rate will increase quickly and the area under the curve will be close to 1. If the classifier is no better than random guessing, the true positive rate will increase linearly with the false positive rate and the area under the curve will be around 0.5.\n",
    "\n",
    "## Project Design\n",
    "After describing the problem, a possible solution and our dataset we would like to spend a couple moments explaining the overall project design. In general the approach will be split up in following chronological steps:\n",
    "\n",
    "### Exploratory Analysis\n",
    "To get a better grasp on the information that is included in the data set, we'll start off with a thorough exploratory analysis. After looking at the key metrics of the data we'll start asking basic questions regarding the status quo of our employees. We'll try to figure out which features tend to be more describing than others when it comes to our prediction tasks and therefore we'll be looking at correlations to discover underlying relationships in our data. \n",
    "\n",
    "### Data Preprocessing\n",
    "From a short preview on the data set we can already tell that we are dealing with numeric but also categorical data. In order to make this information machine-readable we'll be applying feature transformation techniques such as one-hot-encoding. For numeric attributes we might consider normalization or calibrating. In addition, depending on our exploratory analysis, we might consider imputation techniques for missing data as well as outlier detection and processing.\n",
    "\n",
    "### Deciding on algorithms & techniques\n",
    "Even though we already established the core principle of the research as a supervised learning task we'll be conducting additional research to find the best fitting algorithms for our deemed solution. The list of possible algorithms is long and we'll be trying even some more exotic ones in order to get to the best possible solution.\n",
    "There is a large set of possibilities on how to tackle this problem but our key metrices to narrow down the list of algorithms will be data-size, computational efficiency and cross-validation scores on the training set.\n",
    "\n",
    "We will be holding out between 20 and 40 % of the data in order to make sure we're not overfitting. Depending on the amount of features we'll be keeping we might reduce the testing amount in order to make sure to give our model enough data points to train.\n",
    "\n",
    "### Defining Models\n",
    "After deciding on 3 or 4 different supervised algorithms from the set of [Linear SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC), [Logistic Regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), [KNeighbors Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) [polynomial SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC), [Random Forest Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) or [eXtreme Gradient Boosting](https://pypi.python.org/pypi/xgboost). In order to figure out the best way of handling our prediction we'll apply numerous optimization tasks. This might include selecting, fine-tuning, and combining the best algorithms using techniques such as model fitting, model blending, data reduction, feature selection, and assessing the yield of each model, over the baseline. To avoid overfitting and enable generalization we will be using cross-validation. Once we narrowed down the list of possible candidates we'll be applying grid search to tweak the algorithms for the best parameters given the task.\n",
    "\n",
    "### Discussion\n",
    "Finally we'll discuss our model and our results in comparison to mentioned references and benchmark-models. This is, where we want to emphasize the general idea behind our approach and open it up to additional data sets from small to mid-size companies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* [1] **Punnose R, Pankaj A** (2016) \"[Prediction of Employee Turnover in Organizations using Machine Learning Algorithms](https://thesai.org/Downloads/IJARAI/Volume5No9/Paper_4-Prediction_of_Employee_Turnover_in_Organizations.pdf)\" in  *International Journal of Advanced Research in Artificial Intelligence, Vol 5.*\n",
    "\n",
    "* [2] **Weisbeck, D** (2015) \"[Fact or Hype: Do Predictive Workforce Analytics Actually Work?](http://www.visier.com/tech-insights/do-predictive-workforce-analytics-actually-work/)\" on *visier.com*.\n",
    "\n",
    "* [3] **Cotton J, Tuttle J** (1986) \"[Employee Turnover: A Meta-Analysis and Review With Implications for Research](https://www.researchgate.net/publication/211384381_Employee_Turnover_A_Meta-Analysis_and_Review_With_Implications_for_Research)\" in *The Academy of Management Review 11(1)*."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
